{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heavily adapted from: \n",
    "https://github.com/tobyatgithub/bert_tutorial/blob/master/Bert_tutorial1_embeddings.ipynb\n",
    "\n",
    "We will be using pretrained BERT model to go from raw words into latent embeddings\n",
    "#### word -> tokens -> ids -> hidden states -> embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install pytorch torchvision -c pytorch\n",
    "!pip install pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dakaspar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nlp\n",
    "#import nltk\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Brexit’s generational divide is a major fault ...</td>\n",
       "      <td>PORTSMOUTH, England — Eddie Izzard, one of Bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>UK votes for Brexit: What just happened -- and...</td>\n",
       "      <td>London (CNN)As dawn broke over the UK Friday, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Brexit’s generational divide is a major fault ...   \n",
       "1  UK votes for Brexit: What just happened -- and...   \n",
       "\n",
       "                                                body  \n",
       "0  PORTSMOUTH, England — Eddie Izzard, one of Bri...  \n",
       "1  London (CNN)As dawn broke over the UK Friday, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = nlp.load_labels('articles_db.db')\n",
    "corpus = nlp.load_articles('articles_db.db')\n",
    "df = pd.DataFrame(corpus, columns=['title', 'body'])\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['prepped_body'] = df['body'] + \" [SEP]\"\n",
    "df.loc[0, 'prepped_body'] = \"[CLS] \" + df.loc[0, 'prepped_body']\n",
    "df['tokens'] = None\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(df['prepped_body']):\n",
    "    df.loc[i, 'tokens'] = tokenizer.tokenize(text)\n",
    "    df.loc[i, 'tokens'] = df.loc[i, 'tokens'][:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84344\n",
      "511\n"
     ]
    }
   ],
   "source": [
    "# word -> tokens -> ids -> hidden states -> embeddings\n",
    "\n",
    "all_tokens = []\n",
    "input_type_ids = []\n",
    "# masks for segment, 0 for the first sentence, 1 for the second sentence.\n",
    "# use 1 if there's only one sentence.\n",
    "\n",
    "for i, tokens in enumerate(df['tokens']):\n",
    "    for token in tokens:\n",
    "        all_tokens.append(token)\n",
    "        input_type_ids.append(i)\n",
    "print(len(input_type_ids))\n",
    "input_type_ids = input_type_ids[:511]\n",
    "print(len(input_type_ids))\n",
    "# print(\"tokens:\", tokens)   \n",
    "# print(\"type_ids:\", input_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84344"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('things', 101)\n",
      "('fall', 10913)\n",
      "('apart', 1010)\n",
      "('.', 2563)\n",
      "('and', 1517)\n",
      "('now', 5752)\n",
      "(',', 1045)\n",
      "('\"', 20715)\n",
      "('things', 4103)\n",
      "('\"', 1010)\n",
      "('includes', 2028)\n",
      "('the', 1997)\n",
      "('european', 3725)\n",
      "('union', 1521)\n",
      "('.', 1055)\n",
      "('british', 2087)\n",
      "('voters', 3297)\n",
      "('delivered', 25119)\n",
      "('a', 1010)\n",
      "('well', 2003)\n",
      "('-', 2006)\n",
      "('aimed', 1037)\n",
      "('kick', 23624)\n",
      "('at', 2278)\n",
      "('the', 1010)\n"
     ]
    }
   ],
   "source": [
    "# We can only use 512 tokens with BERT\n",
    "input_ids = tokenizer.convert_tokens_to_ids(all_tokens[:511])\n",
    "for pair in zip(tokens[:25], input_ids[:25]):\n",
    "    print(pair)\n",
    "# notice the case ---> uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 10913, 1010, 2563, 1517, 5752, 1045, 20715, 4103, 1010, 2028, 1997, 3725, 1521, 1055, 2087, 3297, 25119, 1010, 2003]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n",
      "\n",
      "[101, 10913, 1010, 2563, 1517, 5752, 1045, 20715, 4103, 1010, 2028, 1997, 3725, 1521, 1055, 2087, 3297, 25119, 1010, 2003]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0]])\n"
     ]
    }
   ],
   "source": [
    "# padding\n",
    "seq_length = 512 # max allowed length & padding length for each pair of sentences. 512\n",
    "input_mask = [1] * len(input_ids)\n",
    "print(input_ids[:20])\n",
    "print(input_mask[:20])\n",
    "print(input_type_ids[:20])\n",
    "while len(input_ids) < seq_length:\n",
    "    input_ids.append(0)\n",
    "    input_mask.append(0)\n",
    "    input_type_ids.append(0)\n",
    "    \n",
    "print()\n",
    "print(input_ids[:20])\n",
    "print(input_mask[:20])\n",
    "print(input_type_ids[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "512\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(len(input_ids))\n",
    "print(len(input_mask))\n",
    "print(len(input_type_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# model = model.cuda()\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1])\n",
      "torch.Size([512, 1])\n",
      "torch.Size([512, 1])\n"
     ]
    }
   ],
   "source": [
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    # ids -> hidden state vectors\n",
    "    input_tensor = torch.LongTensor(input_ids).view(-1,1)\n",
    "    input_mask = torch.LongTensor(input_mask).view(-1,1)\n",
    "    input_type_ids = torch.LongTensor(input_type_ids).view(-1,1)\n",
    "    \n",
    "    print(input_tensor.shape)\n",
    "    print(input_mask.shape)  \n",
    "    print(input_type_ids.shape)\n",
    "    encoded_layers, _ = model(input_tensor, token_type_ids=input_type_ids, attention_mask=input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " torch.Size([512, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "# to get the token embedding vector, we can sum the last four\n",
    "#print(text_a, text_b)\n",
    "sum_last_four = torch.sum(torch.stack(encoded_layers[-4:]), dim=0)\n",
    "print('\\n\\n', sum_last_four.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(torch.cat(encoded_layers[-4:]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------EVERYTHING BELOW IS OLD MATERIAL ---- NOT USED ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(text):\n",
    "\t\"\"\"\n",
    "\tProcesses a text document by coverting all words to lower case,\n",
    "\ttokenizing, removing all non-alphabetical characters,\n",
    "\tand stemming each word.\n",
    "\tArgs:\n",
    "\t\ttext: A string of the text of a single document.\n",
    "\tReturns:\n",
    "\t\tA list of processed words from the document.\n",
    "\t\"\"\"\n",
    "\t# Convert words to lower case\n",
    "\ttext = text.lower()\n",
    "\n",
    "\t# Tokenize corpus and remove all non-alphabetical characters\n",
    "\ttokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\ttokens = tokenizer.tokenize(text)\n",
    "\n",
    "\t# Remove stopwords\n",
    "\tstop_words = nltk.corpus.stopwords.words('english')\n",
    "\tset_stopwords = set(stop_words)\n",
    "\tstopwords_removed = [token for token in tokens if not token in set_stopwords]\n",
    "\n",
    "\t# Stem words\n",
    "\tstemmer = nltk.stem.SnowballStemmer('english')\n",
    "\tstemmed = [stemmer.stem(word) for word in stopwords_removed]\n",
    "\n",
    "\t# Return list of processed words\n",
    "\treturn stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(corpus, columns=['title', 'body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_document(corpus[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
