{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf5Pl0jNvjfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA, KernelPCA, SparsePCA\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwkFxtqDImPh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af2acf25-05db-486f-a4b8-229aa94af5cb"
      },
      "source": [
        "#!pip install nlp\n",
        "\n",
        "!git clone https://github.com/fastai/courses.git\n",
        "#import nlp"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'courses' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELfFVNCR1aZo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "outputId": "58a21c5b-02ec-4ca4-ad33-20a1a5f0a93e"
      },
      "source": [
        "\n",
        "import os\n",
        "import sqlite3\n",
        "\n",
        "\n",
        "scalar = MinMaxScaler()\n",
        "      \n",
        "kmeans = KMeans(n_clusters=64, random_state=0)\n",
        "#x = Scalar.fit_transform(x)\n",
        "pca = SparsePCA(max_iter=10)\n",
        "#pca = KernelPCA(kernel='sigmoid')#n_components=5,\n",
        "#labels = np.linspace(labels)\n",
        "\n",
        "kmeans.fit_predict(data)\n",
        "a = SpectralClustering(n_clusters=64,assign_labels=\"discretize\",random_state=0).fit_predict(data)\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "#n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "n_clusters_ = len(set(a)) - (1 if -1 in a else 0)\n",
        "some_labels = kmeans.labels_\n",
        "\n",
        "\n",
        "\n",
        "def classify_documents(topics, labels):\n",
        "\n",
        "\n",
        "\tpass\n",
        "\n",
        "\n",
        "def cluster_documents(topic, labels,data_dir):\n",
        "\n",
        "\tpass\n",
        "\n",
        "\n",
        "\n",
        "def main(data_dir):\n",
        "\tall_articles = load_articles(data_dir) # list of tuples [title, article]\n",
        "\tlabels = load_labels(data_dir) # hashmap of key: unique doc id, val: title\n",
        "\n",
        "\n",
        "\n",
        "# Run using 'python nlp.py' or 'python nlp.py <PATH_TO_BBC_DIRECTORY>'\n",
        "# to manually specify the path to the data.\n",
        "# This may take a little bit of time (~30-60 seconds) to run.\n",
        "if __name__ == '__main__':\n",
        "\t# data_dir = '/course/cs1951a/pub/nlp/bbc/data' if len(sys.argv) == 1 else sys.argv[1]\n",
        "\tdata_dir = os.path.join(os.getcwd(), 'articles_db.db')\n",
        "\tmain(data_dir)\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-909844c91a4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# data_dir = '/course/cs1951a/pub/nlp/bbc/data' if len(sys.argv) == 1 else sys.argv[1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'articles_db.db'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-909844c91a4c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mall_articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# list of tuples [title, article]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# hashmap of key: unique doc id, val: title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-909844c91a4c>\u001b[0m in \u001b[0;36mload_articles\u001b[0;34m(data_dir)\u001b[0m\n\u001b[1;32m     63\u001b[0m \t\"\"\"\n\u001b[1;32m     64\u001b[0m         \u001b[0mscalar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'articles_db.db'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'articles_db.db'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFvmY2nbU31K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.layers import Input, Dense, Activation, Flatten, BatchNormalization\n",
        "import numpy as np\n",
        "#from numpy import array\n",
        "from keras.models import load_model, Model, model_from_json, Sequential\n",
        "from keras.callbacks import EarlyStopping\n",
        "#next steps to implement this deep learning layer if I cannot implement it in time:\n",
        "\n",
        "\n",
        "#1. assign the index of each of the clusters as the y_train and make sure it corresponds to each vector in x_train\n",
        "#2. use x_test which will be the the new incoming articles to x_test\n",
        "#3. create a schedule function which will have the clustering fire every 6 hours or day or whatever time interval you want \n",
        "#and use the 2nd schedule function to have what is in this cell fire every half hour or hour. Then you are done! The prediction variable in this cell will tell you which of the existing clusters it should go to! \n",
        "\n",
        "#labels = np.array(labels)\n",
        "#x = tf.cast(x, tf.float64)\n",
        "#labels = tf.cast(labels, tf.float64)\n",
        "#x = np.squeeze(x)\n",
        "#labnels = np.squeeze(labels)\n",
        "#x = pd.DataFrame(x)\n",
        "#X_train = data\n",
        "xShape = labels_.shape()\n",
        "xShape\n",
        "#theoretically this should return the indexes of the clusters \n",
        "y_train = labels_[1][0]\n",
        "#what are x_train y_train x_test going to be\n",
        "#x_train are the news articles\n",
        "#y_train are the clusters\n",
        "#x_test are the new news articles\n",
        "inputs = Input(shape=(xShape,))\n",
        "first = LSTM(8, activation='relu')(inputs)\n",
        "A = Dense(250, activation='relu')(first)\n",
        "x = Dense(500, activation='relu')(A)\n",
        "#this batch normalization is to make sure that overfitting does not occur\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(10,activation='relu')(x)\n",
        "\n",
        "#sigmoid is the best optimizer to use for classification tasks supposedly\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model.compile(loss= 'categorical_crossentropy',optimizer='adam')\n",
        "\n",
        "model.fit(X_train,Y,epochs=1200, verbose=1,validation_split=0.2)\n",
        "\n",
        "Prediction = model.predict(x_test)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}